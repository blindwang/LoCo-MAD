model:
  # ModelZoo model name
  arch: mad_qformer_opt
  # model type in model class
  model_type: vitl
  # load pretrained/pretrained: bool and the path
  pretrained: "/data/wjy/workspace/lavis-mm-video-captioning/lavis/output/MAD_Qformer/autoad_caption_lsmdc/20240620101/checkpoint_best.pth"
#  pretrained: "/data/lzh/workspace/lavis-mm-video-captioning/lavis/output/pretrained_weigths/blip2_pretrained_vitL_mad768.pth"
  load_pretrained: True
  load_finetuned: False
  encoder_config:
      end2end: False
      dim_features: 768
  num_query_token: 32
  visual_num_temporal_embedding: 100
#  opt_model: "meta-llama/Llama-2-7b-chat-hf"
  opt_model: "facebook/opt-2.7b"
  prompt: "Generate the description of this clip"
  subtitle: False
  caption: True
  contextual_max_len: 50
  top_k: 0
  num_hidden_layers: 2
  max_txt_len: 36
  fixed_contextual_range: 5

datasets:
  mad_cap_lsmdc:
    data_type: features
    vis_processor:
      train:
          name: "auto_ad_video_train"
#          input_template: [ "clip_vit_L_fp16_features.npz" ]
#          output_keys: [ "feature_visual" ]
          pad_key_lens:
            - [ "feature_visual", 100 ]
          context_range: 5
      eval:
          name: "auto_ad_video_train"
#          input_template: [ "clip_vit_L_fp16_features.npz" ]
#          output_keys: [ "feature_visual" ]
          pad_key_lens:
            - [ "feature_visual", 100 ]
          context_range: 5
    text_processor:
      train:
          name: "blip_caption"
      eval:
          name: "blip_caption"
    build_info:
      annotations:
        train:
#          storage: MAD/annotation_long_unamed.json
#          storage: MAD/mad_annotation_train_with_aug_v3.json
          storage: MAD/LSMDC_multi_sentence_dataset_train.json
        val:
#           storage: MAD/mad_annotation_eval.json
#          storage: MAD/annotation_long_unamed_eval.json
           storage: MAD/LSMDC_multi_sentence_dataset_val.json
        test:
#           storage: MAD/mad_annotation_eval.json
#          storage: MAD/annotation_long_unamed_eval.json
           storage: MAD/LSMDC_multi_sentence_dataset_val.json
      features:
        storage: MAD

run:
  task: mad
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-5
  min_lr: 0
  warmup_lr: 1e-8

  weight_decay: 0.05
  max_epoch: 5
  batch_size_train: 32
  batch_size_eval: 32
  num_workers: 2
  warmup_steps: 500

  max_len: 30
  min_len: 5
  num_beams: 3
  gt_files:
    val: 'MAD/coco/cap_lsmdc_multi_sent_coco_val.json'
    test: 'MAD/coco/cap_lsmdc_multi_sent_coco_val.json'

  seed: 2023
  output_dir: "output/MAD_Qformer/autoad_caption_lsmdc"

  amp: True
  resume_ckpt_path: null

  evaluate: False
  train_splits: ["train"]
  valid_splits: ["val"]

  device: "cuda"
  world_size: 4
  dist_url: "env://"
  distributed: True