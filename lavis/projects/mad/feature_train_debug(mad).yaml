model:
  # ModelZoo model name
  arch: mad_qformer_opt
  # model type in model class
  model_type: vitl
  # load pretrained/pretrained: bool and the path
#  pretrained: "/data/wjy/workspace/lavis-mm-video-captioning/lavis/output/MAD_Qformer/train_qformer_datav3/20240816160/checkpoint_best.pth"
#  pretrained: "/data/wjy/workspace/lavis-mm-video-captioning/lavis/output/MAD_Qformer/autoad_caption_datav3/20240818230/checkpoint_best.pth"
  pretrained: "/data/wjy/workspace/lavis-mm-video-captioning/lavis/output/MAD_Qformer/autoad_caption_datav3/20240627010/checkpoint_best.pth"
#  pretrained: "/data/lzh/workspace/lavis-mm-video-captioning/lavis/output/pretrained_weigths/blip2_pretrained_vitL_mad768.pth"
  load_pretrained: True
  load_finetuned: False
  encoder_config:
      end2end: False
      dim_features: 768
  num_query_token: 32
  visual_num_temporal_embedding: 1000
  opt_model: "facebook/opt-2.7b"
  prompt: "Generate the description of this clip"
  subtitle: True
  caption: True
  contextual_max_len: 200
  top_k: 0
  num_hidden_layers: 2
  max_txt_len: 36
  fixed_contextual_range: 0

datasets:
  mad_cap:
    data_type: features
    vis_processor:
      train:
          name: "auto_ad_video_train"
#          input_template: [ "clip_vit_L_fp16_features.npz" ]
#          output_keys: [ "feature_visual" ]
          pad_key_lens:
            - [ "feature_visual", 1000 ]
          context_range: 5
          char_num: 2
      eval:
          name: "auto_ad_video_train"
#          input_template: [ "clip_vit_L_fp16_features.npz" ]
#          output_keys: [ "feature_visual" ]
          pad_key_lens:
            - [ "feature_visual", 1000 ]
          context_range: 5
          char_num: 2
    text_processor:
      train:
          name: "blip_caption"
      eval:
          name: "blip_caption"
    build_info:
      annotations:
        train:
#          storage: MAD/annotation_long_unamed.json
          storage: MAD/mad_annotation_train_with_aug_v4.json
#          storage: MAD/annotation_long.json
        val:
#          storage: MAD/mad_annotation_eval.json
#          storage: MAD/annotation_long_unamed_eval.json
#           storage: MAD/annotation_long_eval.json
          storage: MAD/mad_annotation_eval_with_aug_v4.json
        test:
#           storage: MAD/mad_annotation_eval.json
#          storage: MAD/annotation_long_unamed_eval.json
          storage: MAD/mad_annotation_eval_with_aug_v4.json
      features:
        storage: MAD

run:
  task: mad
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-5
  min_lr: 0
  warmup_lr: 1e-8

  weight_decay: 0.05
  max_epoch: 2
  batch_size_train: 2
  batch_size_eval: 2
  num_workers: 2
  warmup_steps: 500

  max_len: 36
  min_len: 5
  num_beams: 3
  gt_files:
#    val: 'MAD/coco/cap_lsmdc_coco_harry.json'
#    test: 'MAD/coco/cap_lsmdc_coco_harry.json'
    val: 'MAD/coco/cap_test_lsmdc.json'
    test: 'MAD/coco/cap_test_lsmdc.json'
#    val: 'MAD/coco/cap_test_lsmdc_unamed_coco.json'
#    test: 'MAD/coco/cap_test_lsmdc_unamed_coco.json'
#    val: 'MAD/coco/cap_test_coco.json'
#    test: 'MAD/coco/cap_test_coco.json'

  seed: 2023
  output_dir: "output/MAD_Qformer/autoad_caption_datav4"

  amp: True
  resume_ckpt_path: null

  evaluate: True
  train_splits: ["train"]
  valid_splits: ["val"]

  device: "cuda"
  world_size: 4
  dist_url: "env://"
  distributed: True