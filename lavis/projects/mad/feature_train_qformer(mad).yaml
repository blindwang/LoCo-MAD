model:
  # ModelZoo model name
  arch: mad_qformer
  # model type in model class
  model_type: base
  # load pretrained/pretrained: bool and the path
  pretrained: "/data/lzh/workspace/lavis-mm-video-captioning/lavis/output/pretrained_weigths/blip2_pretrained_vitL_mad768.pth"
#  pretrained: "/data/wjy/workspace/lavis-mm-video-captioning/lavis/output/MAD_Qformer/train_qformer_datav3/20240523161/checkpoint_best.pth"
#  pretrained: "/data/wjy/workspace/lavis-mm-video-captioning/lavis/output/MAD_Qformer/train_qformer_datav3/20240421005/checkpoint_best.pth"
  load_pretrained: True
  visual_num_temporal_embedding: 200
  encoder_config:
      end2end: False
      dim_features: 768
#  cal_itm: False
#  cal_itc: True
#  cal_lm: True
  loss_config:
    LM:
      weight: 1.0
    ITC:
      weight: 1.0
    ITM:
      weight: 1.0
  dim_features: 1408  # 1408
  num_query_token: 64
  audio: False
  audio_dim_features: 128
  audio_max_len: 8


datasets:
  mad_feature_caption:
    data_type: features
    vis_processor:
        train:
          name: "mad_qformer_feature"
          output_keys: [ "feature_visual" ]
          pad_key_lens:
            - [ "feature_visual", 200 ]
        eval:
          name: "mad_qformer_feature"
          output_keys: [ "feature_visual" ]
          pad_key_lens:
            - [ "feature_visual", 200 ]
    text_processor:
        train:
          name: "blip_caption"
        eval:
          name: "blip_caption"
    build_info:
      annotations:
        train:
#          storage: MAD/mad_annotation_train_with_aug_v3.json
#          storage: MAD/LSMDC_multi_sentence_dataset_train.json
            storage: MAD/annotation_long_unamed.json
        val:
#          storage: MAD/LSMDC_multi_sentence_dataset_test.json
#           storage: MAD/mad_annotation_eval.json
          storage: MAD/annotation_long_unamed_eval.json
        test:
#          storage: MAD/LSMDC_multi_sentence_dataset_test.json
#          storage: MAD/mad_annotation_eval.json
          storage: MAD/annotation_long_unamed_eval.json
      features:
        storage: MAD

run:
  task: mad_video_captioning
  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 1e-5
  min_lr: 0
  warmup_lr: 1e-8

  weight_decay: 0.05
  max_epoch: 10
  batch_size_train: 64
  batch_size_eval: 32
  num_workers: 2
  warmup_steps: 500

  max_len: 36
  min_len: 5
  num_beams: 3
  gt_files:
    val: 'MAD/coco/cap_test_lsmdc_unamed_coco.json'
    test: 'MAD/coco/cap_test_lsmdc_unamed_coco.json'
#    val: 'MAD/coco/cap_test_lsmdc.json'
#    test: 'MAD/coco/cap_test_lsmdc.json'
#    val: 'MAD/coco/cap_test_coco.json'
#    test: 'MAD/coco/cap_test_coco.json'
#    val: 'MAD/coco/cap_lsmdc_multi_sent_coco_test.json'
#    test: 'MAD/coco/cap_lsmdc_multi_sent_coco_test.json'

  seed: 2023
  output_dir: "output/MAD_Qformer/train_qformer_datav3"

  amp: True
  resume_ckpt_path: null

  evaluate: False
  train_splits: ["train"]
  valid_splits: ["val"]

  device: "cuda"
  world_size: 4
  dist_url: "env://"
  distributed: True